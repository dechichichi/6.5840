# Lab1 MapReduce

## 编程模型

用户编写Map 函数与 Reduce函数

```
map (k1,v1) --> list(k2,v2)
reduce (k2,list(v2)) --> list (v2)
```

## 实现![1506329-20220923200814531-406767197](https://img2022.cnblogs.com/blog/1506329/202209/1506329-20220923200814531-406767197.png)

### 数据结构

master中维护了一些数据结构。对于每一个map和reduce任务，master存储了对应的任务状态(闲置的，运行中，或者已完成)，以及worker机器的id(针对非空闲的任务)。

master是一个管道，将中间态文件的位置信息从map任务传递给reduce任务。
因此，对于每个已完成的map任务，master存储了由map任务生成的R个中间态文件区域的位置和大小。
当map任务完成时，master将更新接受到的(中间态文件区域)位置和大小信息。
这些信息的变更会以增量的方式推送给运行中的reduce任务。

### 容错

master会周期性的ping每一个worker。
如果在一定的时间内没有接收到来自某一worker的响应，master将会将worker标记为有故障(failed)。
所有由该worker完成的map任务将会被重置回初始状态，因此这些map任务能被其它worker去调度执行。
类似的，任何在这个有故障的worker上处理中的map或reduce任务状态也将被重置为初始化，并且(这些被重置的任务)能够被重新调度执行。

已完成的map任务在故障时需要被重复执行的原因在于map任务的输出是被存储在故障机器的本地磁盘上的，因此无法被访问到(宕机或者网络不通等情况)。
而已完成的reduce任务不需要重复执行的原因在于其输出是被存储在全局的文件系统中的。

当一个map任务在worker A上被首次执行，不久后又被worker B执行(因为worker A发生了故障)，所有执行reduce任务的worker将会被通知需要重新执行。
所有还没有从worker A处读取(完整)数据的reduce任务将改为从worker B处读取数据。

可以简单的让master周期性的将上述的master数据结构以检查点的形式持久化。
如果master任务机器宕机了，一个新的master备份机器将会从最新的检查点状态处启动。
然而，考虑到只有一台master机器，是不太可能出现故障的；因此如果master故障了，我们当前的实现会中止MapReduce计算。
客户端可以检查master的这些状态，并根据需要重试MapReduce操作。

我们依赖map和reduce任务输出结果的原子性提交机制来实现这一特性。
每一个处理中的任务将它们的输出写入其(任务)私有的临时文件中。
一个reduce任务产生一个这样的文件，同时一个map任务产生R个这样的文件(共R个文件，R个reduce任务每个各对应一个文件)。
当一个map任务完成后，对应worker会发送给master一个消息，消息内包含了这R个临时文件名字的。
如果master接受到一个(已被标记为)已完成状态任务的完成消息时，其会忽略该消息。
否则，将这R个文件的名字记录到master(维护)的数据结构中。



当一个reduce任务完成了，执行reduce任务的worker会原子性的将临时的输出文件重命名为最终的输出文件。
如果在多台机器上有相同的reduce任务被执行，在同一个最终输出文件上将会被执行多次重命名调用。
我们依赖底层文件系统所提供的原子性重命名操作来保证最终文件系统中恰好只保存了一次reduce任务执行的数据。